{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "num_epochs = 40#迭代次数\n",
    "hid_units = 256#隐藏层节点个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 辅助的数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def get_all_column_names(predicates):\n",
    "    column_names = set()\n",
    "    for query in predicates:\n",
    "        for predicate in query:\n",
    "            if len(predicate) == 3:\n",
    "                column_name = predicate[0]\n",
    "                column_names.add(column_name)\n",
    "    return column_names\n",
    "\n",
    "\n",
    "def get_all_table_names(tables):\n",
    "    table_names = set()\n",
    "    for query in tables:\n",
    "        for table in query:\n",
    "            table_names.add(table)\n",
    "    return table_names\n",
    "\n",
    "\n",
    "def get_all_operators(predicates):\n",
    "    operators = set()\n",
    "    for query in predicates:\n",
    "        for predicate in query:\n",
    "            if len(predicate) == 3:\n",
    "                operator = predicate[1]\n",
    "                operators.add(operator)\n",
    "    return operators\n",
    "\n",
    "\n",
    "def get_all_joins(joins):\n",
    "    join_set = set()\n",
    "    for query in joins:\n",
    "        for join in query:\n",
    "            join_set.add(join)\n",
    "    return join_set\n",
    "\n",
    "\n",
    "def idx_to_onehot(idx, num_elements):\n",
    "    onehot = np.zeros(num_elements, dtype=np.float32)\n",
    "    onehot[idx] = 1.\n",
    "    return onehot\n",
    "\n",
    "\n",
    "def get_set_encoding(source_set, onehot=True):\n",
    "    num_elements = len(source_set)\n",
    "    source_list = list(source_set)\n",
    "    # Sort list to avoid non-deterministic behavior\n",
    "    source_list.sort()\n",
    "    # Build map from s to i\n",
    "    thing2idx = {s: i for i, s in enumerate(source_list)}\n",
    "    # Build array (essentially a map from idx to s)\n",
    "    idx2thing = [s for i, s in enumerate(source_list)]\n",
    "    if onehot:\n",
    "        thing2vec = {s: idx_to_onehot(i, num_elements) for i, s in enumerate(source_list)}\n",
    "        return thing2vec, idx2thing\n",
    "    return thing2idx, idx2thing\n",
    "\n",
    "\n",
    "def get_min_max_vals(predicates, column_names):\n",
    "    min_max_vals = {t: [float('inf'), float('-inf')] for t in column_names}\n",
    "    for query in predicates:\n",
    "        for predicate in query:\n",
    "            if len(predicate) == 3:\n",
    "                column_name = predicate[0]\n",
    "                val = float(predicate[2])\n",
    "                if val < min_max_vals[column_name][0]:\n",
    "                    min_max_vals[column_name][0] = val\n",
    "                if val > min_max_vals[column_name][1]:\n",
    "                    min_max_vals[column_name][1] = val\n",
    "    return min_max_vals\n",
    "\n",
    "\n",
    "def normalize_data(val, column_name, column_min_max_vals):\n",
    "    min_val = column_min_max_vals[column_name][0]\n",
    "    max_val = column_min_max_vals[column_name][1]\n",
    "    val = float(val)\n",
    "    val_norm = 0.0\n",
    "    if max_val > min_val:\n",
    "        val_norm = (val - min_val) / (max_val - min_val)\n",
    "    return np.array(val_norm, dtype=np.float32)\n",
    "\n",
    "\n",
    "def normalize_labels(labels, min_val=None, max_val=None):\n",
    "    labels = np.array([np.log(float(l)) for l in labels])\n",
    "    if min_val is None:\n",
    "        min_val = labels.min()\n",
    "        print(\"min log(label): {}\".format(min_val))\n",
    "    if max_val is None:\n",
    "        max_val = labels.max()\n",
    "        print(\"max log(label): {}\".format(max_val))\n",
    "    labels_norm = (labels - min_val) / (max_val - min_val)\n",
    "    # Threshold labels\n",
    "    labels_norm = np.minimum(labels_norm, 1)\n",
    "    labels_norm = np.maximum(labels_norm, 0)\n",
    "    return labels_norm, min_val, max_val\n",
    "\n",
    "\n",
    "def unnormalize_labels(labels_norm, min_val, max_val):\n",
    "    labels_norm = np.array(labels_norm, dtype=np.float32)\n",
    "    labels = (labels_norm * (max_val - min_val)) + min_val\n",
    "    return np.array(np.round(np.exp(labels)), dtype=np.int64)\n",
    "\n",
    "\n",
    "def encode_samples(tables, samples, table2vec):\n",
    "    samples_enc = []\n",
    "    for i, query in enumerate(tables):\n",
    "        samples_enc.append(list())\n",
    "        for j, table in enumerate(query):\n",
    "            sample_vec = []\n",
    "            # Append table one-hot vector\n",
    "            sample_vec.append(table2vec[table])\n",
    "            # Append bit vector\n",
    "            sample_vec.append(samples[i][0])\n",
    "            sample_vec = np.hstack(sample_vec)\n",
    "            samples_enc[i].append(sample_vec)\n",
    "    return samples_enc\n",
    "\n",
    "\n",
    "def encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec):\n",
    "    predicates_enc = []\n",
    "    joins_enc = []\n",
    "    for i, query in enumerate(predicates):\n",
    "        predicates_enc.append(list())\n",
    "        joins_enc.append(list())\n",
    "        for predicate in query:\n",
    "            if len(predicate) == 3:\n",
    "                # Proper predicate\n",
    "                column = predicate[0]\n",
    "                operator = predicate[1]\n",
    "                val = predicate[2]\n",
    "                norm_val = normalize_data(val, column, column_min_max_vals)\n",
    "                pred_vec = []\n",
    "                pred_vec.append(column2vec[column])\n",
    "                pred_vec.append(op2vec[operator])\n",
    "                pred_vec.append(norm_val)\n",
    "                pred_vec = np.hstack(pred_vec)\n",
    "            else:\n",
    "                pred_vec = np.zeros((len(column2vec) + len(op2vec) + 1))\n",
    "            predicates_enc[i].append(pred_vec)\n",
    "        for predicate in joins[i]:\n",
    "            # Join instruction\n",
    "            join_vec = join2vec[predicate]\n",
    "            joins_enc[i].append(join_vec)\n",
    "    return predicates_enc, joins_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath, num_materialized_samples,test=False,planrow_path=None):\n",
    "    '''\n",
    "        返回值是下列list,list之中的每一个元素也是一个list，是一个query之中所有的join或table或predicates，使用逗号分隔要素\n",
    "    '''\n",
    "    joins = []\n",
    "    predicates = []\n",
    "    tables = []\n",
    "    samples = []\n",
    "    label = []\n",
    "\n",
    "    # Load queries\n",
    "    with open(filepath, 'rU') as f:\n",
    "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter='#'))\n",
    "        for row in data_raw:\n",
    "            tables.append(row[0].split(','))\n",
    "            joins.append(row[1].split(','))\n",
    "            predicates.append(row[2].split(','))\n",
    "            if test==False:\n",
    "                if int(row[3]) < 1:\n",
    "                    print(\"Queries must have non-zero cardinalities\")\n",
    "                    exit(1)\n",
    "                label.append(row[3])\n",
    "    \n",
    "    if test:\n",
    "        df=pd.read_csv(planrow_path)\n",
    "        label=list(df['planrows'])\n",
    "    print(\"Loaded queries\")\n",
    "\n",
    "    # Load bitmaps\n",
    "    load_bitmaps='''\n",
    "    num_bytes_per_bitmap = int((num_materialized_samples + 7) >> 3)\n",
    "    with open(file_name + \".bitmaps\", 'rb') as f:\n",
    "        for i in range(len(tables)):\n",
    "            four_bytes = f.read(4)\n",
    "            if not four_bytes:\n",
    "                print(\"Error while reading 'four_bytes'\")\n",
    "                exit(1)\n",
    "            num_bitmaps_curr_query = int.from_bytes(four_bytes, byteorder='little')\n",
    "            bitmaps = np.empty((num_bitmaps_curr_query, num_bytes_per_bitmap * 8), dtype=np.uint8)\n",
    "            for j in range(num_bitmaps_curr_query):\n",
    "                # Read bitmap\n",
    "                bitmap_bytes = f.read(num_bytes_per_bitmap)\n",
    "                if not bitmap_bytes:\n",
    "                    print(\"Error while reading 'bitmap_bytes'\")\n",
    "                    exit(1)\n",
    "                bitmaps[j] = np.unpackbits(np.frombuffer(bitmap_bytes, dtype=np.uint8))\n",
    "            samples.append(bitmaps)\n",
    "    print(\"Loaded bitmaps\")\n",
    "    '''\n",
    "    pseudo_samples=[]\n",
    "    pseudo_bitmap=np.zeros((1,num_materialized_samples),dtype=np.uint8)\n",
    "    for _ in range(len(tables)):\n",
    "        pseudo_samples.append(pseudo_bitmap)\n",
    "\n",
    "    # Split predicates\n",
    "    predicates = [list(chunks(d, 3)) for d in predicates]\n",
    "\n",
    "    return joins, predicates, tables, pseudo_samples, label\n",
    "\n",
    "\n",
    "def load_and_encode_train_data(num_queries, num_materialized_samples):\n",
    "    train_query_path = \"./training_data.csv\"\n",
    "    column_minmax_path = \"./column_min_max_vals.csv\"\n",
    "\n",
    "    joins, predicates, tables, samples, label = load_data(train_query_path, num_materialized_samples)\n",
    "\n",
    "    # Get column name dict\n",
    "    column_names = get_all_column_names(predicates)\n",
    "    column2vec, idx2column = get_set_encoding(column_names)\n",
    "\n",
    "    # Get table name dict\n",
    "    table_names = get_all_table_names(tables)\n",
    "    table2vec, idx2table = get_set_encoding(table_names)\n",
    "\n",
    "    # Get operator name dict\n",
    "    operators = get_all_operators(predicates)\n",
    "    op2vec, idx2op = get_set_encoding(operators)\n",
    "\n",
    "    # Get join name dict\n",
    "    join_set = get_all_joins(joins)\n",
    "    join2vec, idx2join = get_set_encoding(join_set)\n",
    "\n",
    "    # Get min and max values for each column\n",
    "    with open(column_minmax_path, 'rU') as f:\n",
    "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter=','))\n",
    "        column_min_max_vals = {}\n",
    "        for i, row in enumerate(data_raw):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            column_min_max_vals[row[0]] = [float(row[1]), float(row[2])]\n",
    "\n",
    "    # Get feature encoding and proper normalization\n",
    "    samples_enc = encode_samples(tables, samples, table2vec)\n",
    "    predicates_enc, joins_enc = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
    "    label_norm, min_val, max_val = normalize_labels(label)\n",
    "\n",
    "    # Split in training and validation samples\n",
    "    num_train = int(num_queries * 0.9)\n",
    "    num_test = num_queries - num_train\n",
    "\n",
    "    samples_train = samples_enc[:num_train]\n",
    "    predicates_train = predicates_enc[:num_train]\n",
    "    joins_train = joins_enc[:num_train]\n",
    "    labels_train = label_norm[:num_train]\n",
    "\n",
    "    samples_test = samples_enc[num_train:num_train + num_test]\n",
    "    predicates_test = predicates_enc[num_train:num_train + num_test]\n",
    "    joins_test = joins_enc[num_train:num_train + num_test]\n",
    "    labels_test = label_norm[num_train:num_train + num_test]\n",
    "\n",
    "    print(\"Number of training samples: {}\".format(len(labels_train)))\n",
    "    print(\"Number of validation samples: {}\".format(len(labels_test)))\n",
    "\n",
    "    max_num_joins = max(max([len(j) for j in joins_train]), max([len(j) for j in joins_test]))\n",
    "    max_num_predicates = max(max([len(p) for p in predicates_train]), max([len(p) for p in predicates_test]))\n",
    "\n",
    "    dicts = [table2vec, column2vec, op2vec, join2vec]\n",
    "    train_data = [samples_train, predicates_train, joins_train]\n",
    "    test_data = [samples_test, predicates_test, joins_test]\n",
    "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data\n",
    "\n",
    "def make_dataset(samples, predicates, joins, labels, max_num_joins, max_num_predicates):\n",
    "    \"\"\"Add zero-padding and wrap as tensor dataset.\"\"\"\n",
    "\n",
    "    sample_masks = []\n",
    "    sample_tensors = []\n",
    "    for sample in samples:\n",
    "        sample_tensor = np.vstack(sample)\n",
    "        num_pad = max_num_joins + 1 - sample_tensor.shape[0]\n",
    "        sample_mask = np.ones_like(sample_tensor).mean(1, keepdims=True)\n",
    "        sample_tensor = np.pad(sample_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
    "        sample_mask = np.pad(sample_mask, ((0, num_pad), (0, 0)), 'constant')\n",
    "        sample_tensors.append(np.expand_dims(sample_tensor, 0))\n",
    "        sample_masks.append(np.expand_dims(sample_mask, 0))\n",
    "    sample_tensors = np.vstack(sample_tensors)\n",
    "    sample_tensors = torch.FloatTensor(sample_tensors)\n",
    "    sample_masks = np.vstack(sample_masks)\n",
    "    sample_masks = torch.FloatTensor(sample_masks)\n",
    "\n",
    "    predicate_masks = []\n",
    "    predicate_tensors = []\n",
    "    for predicate in predicates:\n",
    "        predicate_tensor = np.vstack(predicate)\n",
    "        num_pad = max_num_predicates - predicate_tensor.shape[0]\n",
    "        predicate_mask = np.ones_like(predicate_tensor).mean(1, keepdims=True)\n",
    "        predicate_tensor = np.pad(predicate_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
    "        predicate_mask = np.pad(predicate_mask, ((0, num_pad), (0, 0)), 'constant')\n",
    "        predicate_tensors.append(np.expand_dims(predicate_tensor, 0))\n",
    "        predicate_masks.append(np.expand_dims(predicate_mask, 0))\n",
    "    predicate_tensors = np.vstack(predicate_tensors)\n",
    "    predicate_tensors = torch.FloatTensor(predicate_tensors)\n",
    "    predicate_masks = np.vstack(predicate_masks)\n",
    "    predicate_masks = torch.FloatTensor(predicate_masks)\n",
    "\n",
    "    join_masks = []\n",
    "    join_tensors = []\n",
    "    for join in joins:\n",
    "        join_tensor = np.vstack(join)\n",
    "        num_pad = max_num_joins - join_tensor.shape[0]\n",
    "        join_mask = np.ones_like(join_tensor).mean(1, keepdims=True)\n",
    "        join_tensor = np.pad(join_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
    "        join_mask = np.pad(join_mask, ((0, num_pad), (0, 0)), 'constant')\n",
    "        join_tensors.append(np.expand_dims(join_tensor, 0))\n",
    "        join_masks.append(np.expand_dims(join_mask, 0))\n",
    "    join_tensors = np.vstack(join_tensors)\n",
    "    join_tensors = torch.FloatTensor(join_tensors)\n",
    "    join_masks = np.vstack(join_masks)\n",
    "    join_masks = torch.FloatTensor(join_masks)\n",
    "\n",
    "    target_tensor = torch.FloatTensor(labels)\n",
    "\n",
    "    return dataset.TensorDataset(sample_tensors, predicate_tensors, join_tensors, target_tensor, sample_masks,\n",
    "                                 predicate_masks, join_masks)\n",
    "\n",
    "\n",
    "def get_train_datasets(num_queries, num_materialized_samples):\n",
    "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = load_and_encode_train_data(\n",
    "        num_queries, num_materialized_samples)\n",
    "    train_dataset = make_dataset(*train_data, labels=labels_train, max_num_joins=max_num_joins,\n",
    "                                 max_num_predicates=max_num_predicates)\n",
    "    print(\"Created TensorDataset for training data\")\n",
    "    test_dataset = make_dataset(*test_data, labels=labels_test, max_num_joins=max_num_joins,\n",
    "                                max_num_predicates=max_num_predicates)\n",
    "    print(\"Created TensorDataset for validation data\")\n",
    "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 设置必要的神经网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetConv(nn.Module):\n",
    "    def __init__(self, sample_feats, predicate_feats, join_feats, hid_units):\n",
    "        super(SetConv, self).__init__()\n",
    "        self.sample_mlp1 = nn.Linear(sample_feats, hid_units)\n",
    "        self.sample_mlp2 = nn.Linear(hid_units, hid_units)\n",
    "        self.sample_mlp3 = nn.Linear(hid_units, hid_units)####1.7\n",
    "        self.predicate_mlp1 = nn.Linear(predicate_feats, hid_units)\n",
    "        self.predicate_mlp2 = nn.Linear(hid_units, hid_units)\n",
    "        self.predicate_mlp3 = nn.Linear(hid_units, hid_units)####1.7\n",
    "        self.join_mlp1 = nn.Linear(join_feats, hid_units)\n",
    "        self.join_mlp2 = nn.Linear(hid_units, hid_units)\n",
    "        self.join_mlp3 = nn.Linear(hid_units, hid_units)####1.7\n",
    "        self.out_mlp1 = nn.Linear(hid_units * 3, hid_units)\n",
    "        #self.out_mlp2 = nn.Linear(hid_units, 1)\n",
    "        self.out_mlp2 = nn.Linear(hid_units, hid_units)####1.7\n",
    "        self.out_mlp3 = nn.Linear(hid_units, 1)####1.7\n",
    "\n",
    "    def forward(self, samples, predicates, joins, sample_mask, predicate_mask, join_mask):\n",
    "        # samples has shape [batch_size x num_joins+1 x sample_feats]\n",
    "        # predicates has shape [batch_size x num_predicates x predicate_feats]\n",
    "        # joins has shape [batch_size x num_joins x join_feats]\n",
    "\n",
    "        hid_sample = F.relu(self.sample_mlp1(samples))\n",
    "        hid_sample = F.relu(self.sample_mlp2(hid_sample))\n",
    "        hid_sample = F.relu(self.sample_mlp3(hid_sample))####1.7\n",
    "        hid_sample = hid_sample * sample_mask  # Mask\n",
    "        hid_sample = torch.sum(hid_sample, dim=1, keepdim=False)\n",
    "        sample_norm = sample_mask.sum(1, keepdim=False)\n",
    "        hid_sample = hid_sample / sample_norm  # Calculate average only over non-masked parts\n",
    "\n",
    "        hid_predicate = F.relu(self.predicate_mlp1(predicates))\n",
    "        hid_predicate = F.relu(self.predicate_mlp2(hid_predicate))\n",
    "        hid_predicate = F.relu(self.predicate_mlp3(hid_predicate))####1.7\n",
    "        hid_predicate = hid_predicate * predicate_mask\n",
    "        hid_predicate = torch.sum(hid_predicate, dim=1, keepdim=False)\n",
    "        predicate_norm = predicate_mask.sum(1, keepdim=False)\n",
    "        hid_predicate = hid_predicate / predicate_norm\n",
    "\n",
    "        hid_join = F.relu(self.join_mlp1(joins))\n",
    "        hid_join = F.relu(self.join_mlp2(hid_join))\n",
    "        hid_join = F.relu(self.join_mlp3(hid_join))####1.7\n",
    "        hid_join = hid_join * join_mask\n",
    "        hid_join = torch.sum(hid_join, dim=1, keepdim=False)\n",
    "        join_norm = join_mask.sum(1, keepdim=False)\n",
    "        hid_join = hid_join / join_norm\n",
    "\n",
    "        hid = torch.cat((hid_sample, hid_predicate, hid_join), 1)\n",
    "        hid = F.relu(self.out_mlp1(hid))\n",
    "        hid = F.relu(self.out_mlp2(hid))####1.7\n",
    "        #out = torch.sigmoid(self.out_mlp2(hid))\n",
    "        out = torch.sigmoid(self.out_mlp3(hid))####1.7\n",
    "        return out\n",
    "    \n",
    "def unnormalize_torch(vals, min_val, max_val):\n",
    "    vals = (vals * (max_val - min_val)) + min_val\n",
    "    return torch.exp(vals)\n",
    "\n",
    "\n",
    "def qerror_loss(preds, targets, min_val, max_val):\n",
    "    qerror = []\n",
    "    preds = unnormalize_torch(preds, min_val, max_val)\n",
    "    targets = unnormalize_torch(targets, min_val, max_val)\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
    "            qerror.append(preds[i] / targets[i])\n",
    "        else:\n",
    "            qerror.append(targets[i] / preds[i])\n",
    "    return torch.mean(torch.cat(qerror))\n",
    "\n",
    "\n",
    "def predict(model, data_loader, cuda=False):\n",
    "    preds = []\n",
    "    t_total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, data_batch in enumerate(data_loader):\n",
    "\n",
    "        samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
    "\n",
    "        if cuda:\n",
    "            samples, predicates, joins, targets = samples.cuda(), predicates.cuda(), joins.cuda(), targets.cuda()\n",
    "            sample_masks, predicate_masks, join_masks = sample_masks.cuda(), predicate_masks.cuda(), join_masks.cuda()\n",
    "        samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(\n",
    "            targets)\n",
    "        sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(\n",
    "            join_masks)\n",
    "\n",
    "        t = time.time()\n",
    "        outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
    "        t_total += time.time() - t\n",
    "\n",
    "        for i in range(outputs.data.shape[0]):\n",
    "            preds.append(outputs.data[i])\n",
    "\n",
    "    return preds, t_total\n",
    "\n",
    "\n",
    "def print_qerror(preds_unnorm, labels_unnorm):\n",
    "    qerror = []\n",
    "    for i in range(len(preds_unnorm)):\n",
    "        if preds_unnorm[i] > float(labels_unnorm[i]):\n",
    "            qerror.append(preds_unnorm[i] / float(labels_unnorm[i]))\n",
    "        else:\n",
    "            qerror.append(float(labels_unnorm[i]) / float(preds_unnorm[i]))\n",
    "\n",
    "    print(\"Median: {}\".format(np.median(qerror)))\n",
    "    print(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
    "    print(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
    "    print(\"99th percentile: {}\".format(np.percentile(qerror, 99)))\n",
    "    print(\"Max: {}\".format(np.max(qerror)))\n",
    "    print(\"Mean: {}\".format(np.mean(qerror)))\n",
    "\n",
    "\n",
    "def train_model(num_queries=100000, num_epochs=20, batch_size=1024, hid_units=256, cuda=False):\n",
    "    # Load training and validation data\n",
    "    num_materialized_samples = 1000\n",
    "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(\n",
    "        num_queries, num_materialized_samples)\n",
    "    table2vec, column2vec, op2vec, join2vec = dicts\n",
    "\n",
    "    # Train model\n",
    "    sample_feats = len(table2vec) + num_materialized_samples\n",
    "    predicate_feats = len(column2vec) + len(op2vec) + 1\n",
    "    join_feats = len(join2vec)\n",
    "\n",
    "    model = SetConv(sample_feats, predicate_feats, join_feats, hid_units)\n",
    "\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.003)####1.7\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_total = 0.\n",
    "\n",
    "        for batch_idx, data_batch in enumerate(train_data_loader):\n",
    "\n",
    "            samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
    "\n",
    "            if cuda:\n",
    "                samples, predicates, joins, targets = samples.cuda(), predicates.cuda(), joins.cuda(), targets.cuda()\n",
    "                sample_masks, predicate_masks, join_masks = sample_masks.cuda(), predicate_masks.cuda(), join_masks.cuda()\n",
    "            samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(\n",
    "                targets)\n",
    "            sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(\n",
    "                join_masks)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
    "            loss = qerror_loss(outputs, targets.float(), min_val, max_val)\n",
    "            loss_total += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch {}, loss: {}\".format(epoch, loss_total / len(train_data_loader)))\n",
    "\n",
    "    # Get final training and validation set predictions\n",
    "    preds_train, t_total = predict(model, train_data_loader, cuda)\n",
    "    print(\"Prediction time per training sample: {}\".format(t_total / len(labels_train) * 1000))\n",
    "\n",
    "    preds_test, t_total = predict(model, test_data_loader, cuda)\n",
    "    print(\"Prediction time per validation sample: {}\".format(t_total / len(labels_test) * 1000))\n",
    "\n",
    "    # Unnormalize\n",
    "    preds_train_unnorm = unnormalize_labels(preds_train, min_val, max_val)\n",
    "    labels_train_unnorm = unnormalize_labels(labels_train, min_val, max_val)\n",
    "\n",
    "    preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
    "    labels_test_unnorm = unnormalize_labels(labels_test, min_val, max_val)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"\\nQ-Error training set:\")\n",
    "    print_qerror(preds_train_unnorm, labels_train_unnorm)\n",
    "\n",
    "    print(\"\\nQ-Error validation set:\")\n",
    "    print_qerror(preds_test_unnorm, labels_test_unnorm)\n",
    "    print(\"\")\n",
    "    return model\n",
    "\n",
    "    # show be divided into test function!\n",
    "    # Load test data\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "\n",
    "    '''\n",
    "    # Print metrics\n",
    "    print(\"\\nQ-Error \" + workload_name + \":\")\n",
    "    print_qerror(preds_test_unnorm, label)\n",
    "\n",
    "    # Write predictions\n",
    "    file_name = \"results/predictions_\" + workload_name + \".csv\"\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    with open(file_name, \"w\") as f:\n",
    "        for i in range(len(preds_test_unnorm)):\n",
    "            f.write(str(preds_test_unnorm[i]) + \",\" + label[i] + \"\\n\")\n",
    "    '''\n",
    "def make_prediction(testfilepath,column_minmax_path,planrow_path,num_materialized_samples=1000,batch_size=1024):\n",
    "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(\n",
    "        num_queries=100000, num_materialized_samples=1000)\n",
    "    table2vec, column2vec, op2vec, join2vec = dicts\n",
    "    joins, predicates, tables, samples, label = load_data(testfilepath, num_materialized_samples)#,test=True,planrow_path=planrow_path)\n",
    "    # Get feature encoding and proper normalization\n",
    "    samples_test = encode_samples(tables, samples, table2vec)\n",
    "    \n",
    "    predicates_test, joins_test = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
    "    labels_test, _, _ = normalize_labels(label, min_val, max_val)\n",
    "\n",
    "    print(\"Number of test samples: {}\".format(len(labels_test)))\n",
    "\n",
    "    max_num_predicates = max([len(p) for p in predicates_test])\n",
    "    max_num_joins = max([len(j) for j in joins_test])\n",
    "\n",
    "    # Get test set predictions\n",
    "    test_data = make_dataset(samples_test, predicates_test, joins_test, labels_test, max_num_joins, max_num_predicates)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    preds_test, t_total = predict(model, test_data_loader)\n",
    "    print(\"Prediction time per test sample: {}\".format(t_total / len(labels_test) * 1000))\n",
    "\n",
    "    # Unnormalize\n",
    "    preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
    "    return preds_test_unnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-afdf991ca307>:12: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(filepath, 'rU') as f:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-afdf991ca307>:84: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(column_minmax_path, 'rU') as f:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min log(label): 0.0\n",
      "max log(label): 19.94772801931604\n",
      "Number of training samples: 90000\n",
      "Number of validation samples: 10000\n",
      "Created TensorDataset for training data\n",
      "Created TensorDataset for validation data\n",
      "Epoch 0, loss: 273.50048897483134\n",
      "Epoch 1, loss: 109.84207634492354\n",
      "Epoch 2, loss: 84.12902888384733\n",
      "Epoch 3, loss: 61.02830990878019\n",
      "Epoch 4, loss: 43.965726288882166\n",
      "Epoch 5, loss: 38.51639082215049\n",
      "Epoch 6, loss: 29.181722510944713\n",
      "Epoch 7, loss: 31.274290496652778\n",
      "Epoch 8, loss: 30.444614497098055\n",
      "Epoch 9, loss: 21.365904743021186\n",
      "Epoch 10, loss: 19.63683440468528\n",
      "Epoch 11, loss: 17.22198265249079\n",
      "Epoch 12, loss: 17.531617587262932\n",
      "Epoch 13, loss: 13.38899337161671\n",
      "Epoch 14, loss: 13.116391989317806\n",
      "Epoch 15, loss: 11.461440964178605\n",
      "Epoch 16, loss: 11.036223406141454\n",
      "Epoch 17, loss: 13.465761629017917\n",
      "Epoch 18, loss: 13.716177582740784\n",
      "Epoch 19, loss: 10.044799208641052\n",
      "Epoch 20, loss: 9.105836711146615\n",
      "Epoch 21, loss: 8.976868748664856\n",
      "Epoch 22, loss: 8.528305888175964\n",
      "Epoch 23, loss: 8.261263576420872\n",
      "Epoch 24, loss: 317.80756109411067\n",
      "Epoch 25, loss: 89.30358024076982\n",
      "Epoch 26, loss: 41.66900209947066\n",
      "Epoch 27, loss: 28.984847220507536\n",
      "Epoch 28, loss: 24.779051488096062\n",
      "Epoch 29, loss: 19.869987401095305\n",
      "Epoch 30, loss: 15.081879128109325\n",
      "Epoch 31, loss: 13.182909683747726\n",
      "Epoch 32, loss: 11.294289301742207\n",
      "Epoch 33, loss: 10.426651380278848\n",
      "Epoch 34, loss: 9.701567552306436\n",
      "Epoch 35, loss: 9.3999019427733\n",
      "Epoch 36, loss: 8.766798111525448\n",
      "Epoch 37, loss: 8.2169185443358\n",
      "Epoch 38, loss: 7.826750153845007\n",
      "Epoch 39, loss: 8.660217306830667\n",
      "Prediction time per training sample: 0.0404171864191691\n",
      "Prediction time per validation sample: 0.02562248706817627\n",
      "\n",
      "Q-Error training set:\n",
      "Median: 2.1923076923076925\n",
      "90th percentile: 10.784360902255647\n",
      "95th percentile: 22.38562651148981\n",
      "99th percentile: 122.50049450549425\n",
      "Max: 14048.666666666666\n",
      "Mean: 10.602806484239673\n",
      "\n",
      "Q-Error validation set:\n",
      "Median: 2.219708640981433\n",
      "90th percentile: 11.364772727272731\n",
      "95th percentile: 25.0\n",
      "99th percentile: 136.00600828729296\n",
      "Max: 29495.0\n",
      "Mean: 17.485662339724357\n",
      "\n",
      "save success!\n"
     ]
    }
   ],
   "source": [
    "model=train_model(num_epochs=num_epochs, hid_units=hid_units)\n",
    "torch.save(model,'./model/model_'+str(num_epochs)+'_'+str(hid_units)+'.pth')\n",
    "print(\"save success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-afdf991ca307>:12: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(filepath, 'rU') as f:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-afdf991ca307>:84: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(column_minmax_path, 'rU') as f:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min log(label): 0.0\n",
      "max log(label): 19.94772801931604\n",
      "Number of training samples: 90000\n",
      "Number of validation samples: 10000\n",
      "Created TensorDataset for training data\n",
      "Created TensorDataset for validation data\n",
      "Loaded queries\n",
      "Number of test samples: 5000\n",
      "Prediction time per test sample: 0.021930599212646482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([       41,    537198, 149364096, ...,       146,      4027,\n",
       "         3561595])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result=make_prediction('./synthetic.csv','./column_min_max_vals.csv','planrow.csv')\n",
    "pred_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 得到query plan中对规模的预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern=r'rows=(\\d*)'\n",
    "planrows=[]\n",
    "for i in range(5000):\n",
    "    with open('./testing_plans/'+str(i)+'.txt',mode='r') as f:\n",
    "        line=f.readline()\n",
    "    match=re.search(pattern,line)\n",
    "    if match:\n",
    "        planrows.append(int(match.group(1)))\n",
    "    else:\n",
    "        planrows.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('planrow.txt',mode='w')as f:\n",
    "    f.write('queryid'+','+'planrows'+'\\n')\n",
    "    for i in range(5000):\n",
    "        f.write(str(i)+','+str(planrows[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "with open('./result/pred_'+str(num_epochs)+'_'+str(hid_units)+'.csv',mode='w') as f:\n",
    "    f.write('Query ID'+','+'Predicted Cardinality'+'\\n')\n",
    "    for i in range(len(pred_result)):\n",
    "        #f.write(str(i)+','+str(pred_result[i])+'\\n')\n",
    "        f.write(str(i)+','+str(pred_result[i])+'\\n')\n",
    "print(\"Success!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
